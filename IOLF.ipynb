{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importations"
      ],
      "metadata": {
        "id": "DR9guFSVubkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niOruq6h3Nlx",
        "outputId": "1cdf2935-30d1-433a-da7c-89824fbc1f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.0)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.29.35)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.16)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n",
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pmdarima\n",
        "!pip install XlsxWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A0VimWuY1VbK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import Bounds, minimize, SR1, LinearConstraint\n",
        "from pandas import read_csv, DataFrame, concat\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import pandas as pd\n",
        "import pmdarima as pm\n",
        "from copy import deepcopy\n",
        "import scipy.optimize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xlsxwriter as xls\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#from LSTMseries import LSTMmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "A7JcXRrluiSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9Rvuz1zo1jGo"
      },
      "outputs": [],
      "source": [
        "# transform a time series dataset into a supervised learning dataset\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "        df = DataFrame(data)\n",
        "        cols = list()\n",
        "        # input sequence (t-n, ... t-1)\n",
        "        for i in range(n_in, 0, -1):\n",
        "                cols.append(df.shift(i))\n",
        "\t\t# forecast sequence (t, t+1, ... t+n)\n",
        "        for i in range(0, n_out):\n",
        "                cols.append(df.shift(-i))\n",
        "        # put it all together\n",
        "        agg = concat(cols, axis=1)\n",
        "        # drop rows with NaN values\n",
        "        if dropnan:\n",
        "                agg.dropna(inplace=True)\n",
        "        return agg.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SpQ1T_M62L0_"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Prepare Training Data\n",
        "\n",
        "For this example, we will use sliding windows of 10 points per each window (equ>\n",
        "\n",
        "Min-Max scaling has also been fitted to the training data to aid the convergenc\n",
        "\"\"\"\n",
        "\n",
        "#def inverse_transform(y):\n",
        "#  return target_scaler.inverse_transform(y.reshape(-1, 1))\n",
        "\n",
        "def create_sliding_window(data, sequence_length, stride=1):\n",
        "    X_list, y_list = [], []\n",
        "    for i in range(len(data)):\n",
        "      if (i + sequence_length) < len(data):\n",
        "        X_list.append(data.iloc[i:i+sequence_length:stride, :].values)\n",
        "        y_list.append(data.iloc[i+sequence_length, -1])\n",
        "    return np.array(X_list), np.array(y_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zrtrpu7m2h2I"
      },
      "outputs": [],
      "source": [
        "class BayesianLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, output_length, batch_size):\n",
        "\n",
        "        super(BayesianLSTM, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size # user-defined\n",
        "\n",
        "        self.hidden_size_1 = 256 # number of encoder cells (from paper)\n",
        "        self.hidden_size_2 = 64 # number of decoder cells (from paper)\n",
        "        self.stacked_layers = 1 # number of (stacked) LSTM layers for each stage\n",
        "        self.dropout_probability = 0 # arbitrary value (the paper suggests that>\n",
        "\n",
        "        '''self.lstm1 = SFLSTM(input_size = n_features,\n",
        "                            hidden_size = self.hidden_size_1,\n",
        "                            num_layers=self.stacked_layers,\n",
        "                            cell_size = self.hidden_size_1)'''\n",
        "        self.lstm1 = nn.LSTM(n_features,\n",
        "                             self.hidden_size_1,\n",
        "                             num_layers=self.stacked_layers,\n",
        "                             batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(self.hidden_size_1, self.hidden_size_2, num_layers=self.stacked_layers,\n",
        "                             batch_first=True)\n",
        "        self.fc = nn.Linear(self.hidden_size_2, output_length)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        hidden = self.init_hidden1(batch_size)\n",
        "        output, _ = self.lstm1(x, hidden)\n",
        "        output = F.dropout(output, p=self.dropout_probability, training=True)\n",
        "\n",
        "        state = self.init_hidden2(batch_size)\n",
        "        output, state = self.lstm2(output, state)\n",
        "        output = F.dropout(output, p=self.dropout_probability, training=True)\n",
        "\n",
        "        output = output[:, -1, :] # take the last decoder cell's outputs\n",
        "        y_pred = self.fc(output)\n",
        "        return y_pred\n",
        "\n",
        "    def init_hidden1(self, batch_size):\n",
        "        hidden_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_1))\n",
        "        cell_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_1))\n",
        "        return hidden_state, cell_state\n",
        "\n",
        "    def init_hidden2(self, batch_size):\n",
        "        hidden_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_2))\n",
        "        cell_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_2))\n",
        "        return hidden_state, cell_state\n",
        "\n",
        "    def loss(self, pred, truth):\n",
        "        return self.loss_fn(pred, truth)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self(torch.tensor(X, dtype=torch.float32)).view(-1).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RS2zrAuF4egn"
      },
      "outputs": [],
      "source": [
        "class BayesianDoubleLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, output_length, batch_size):\n",
        "\n",
        "        super(BayesianDoubleLSTM, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size # user-defined\n",
        "\n",
        "        self.hidden_size_1 = 64 # number of encoder cells (from paper)\n",
        "        self.hidden_size_2 = 64 # number of decoder cells (from paper)\n",
        "        self.stacked_layers = 1 # number of (stacked) LSTM layers for each stage\n",
        "        self.dropout_probability = 0 # arbitrary value (the paper suggests that>\n",
        "\n",
        "        self.lstm1 = nn.LSTM(n_features, self.hidden_size_1, num_layers=self.stacked_layers, batch_first=True)\n",
        "\n",
        "        # self.lstm2 = nn.LSTM(self.hidden_size_1, self.hidden_size_2, num_layers=self.stacked_layers, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size_1, output_length)\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        hidden = self.init_hidden1(batch_size)\n",
        "        output, _ = self.lstm1(x, hidden)\n",
        "        output = F.dropout(output, p=self.dropout_probability, training=True)\n",
        "\n",
        "        # state = self.init_hidden2(batch_size)\n",
        "        # output, state = self.lstm2(output, state)\n",
        "        # output = F.dropout(output, p=self.dropout_probability, training=True)\n",
        "\n",
        "        output = output[:, -1, :] # take the last decoder cell's outputs\n",
        "        y_pred = self.fc(output)\n",
        "        return y_pred\n",
        "\n",
        "    def init_hidden1(self, batch_size):\n",
        "        hidden_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_1))\n",
        "        cell_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_1))\n",
        "        return hidden_state, cell_state\n",
        "\n",
        "    def init_hidden2(self, batch_size):\n",
        "        hidden_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_2))\n",
        "        cell_state = Variable(torch.zeros(self.stacked_layers, batch_size, self.hidden_size_2))\n",
        "        return hidden_state, cell_state\n",
        "\n",
        "    def loss(self, pred, truth):\n",
        "        return self.loss_fn(pred, truth)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self(torch.tensor(X, dtype=torch.float32)).view(-1).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3qqFdVWa2i0n"
      },
      "outputs": [],
      "source": [
        "def LSTMmodel(filename):\n",
        "\n",
        "    resample_df = pd.read_csv(filename)\n",
        "    #resample_df = resample_df['tcm1yd']\n",
        "    #resample_df = energy_df['Appliances']\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(resample_df))\n",
        "    n_test = len(resample_df) - n_train\n",
        "\n",
        "\n",
        "    feature_array = resample_df.values\n",
        "\n",
        "    if str(type(feature_array[0]))!=\"<class 'numpy.ndarray'>\":\n",
        "        feature_array = np.expand_dims(feature_array, axis = -1)\n",
        "\n",
        "\t  # Fit Scaler only on Training features\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    feature_scaler.fit(feature_array[:n_train])\n",
        "    # Fit Scaler only on Training target values\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(feature_array[:n_train, -1].reshape(-1, 1))\n",
        "\n",
        "    # Transfom on both Training and Test data\n",
        "    scaled_array = pd.DataFrame(feature_scaler.transform(feature_array))\n",
        "\n",
        "    sequence_length = 10\n",
        "    X, y = create_sliding_window(scaled_array, sequence_length)\n",
        "\n",
        "    X_train = X[:n_train]\n",
        "    y_train = y[:n_train]\n",
        "    X_test = X[n_train:]\n",
        "    y_test = y[n_train:]\n",
        "\n",
        "\n",
        "    #el mejor hasta ahora 256-32 (x2) dropout=0, lr=0.001, 1000 eps (0.039)\n",
        "\n",
        "    n_features = scaled_array.shape[-1]\n",
        "    sequence_length = 10\n",
        "    output_length = 1\n",
        "\n",
        "    batch_size = 128\n",
        "    n_epochs = 25\n",
        "    learning_rate = 0.01\n",
        "    #weight_decay = learning_rate/n_epochs\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    bayesian_lstm = BayesianLSTM(n_features=n_features,\n",
        "                                 output_length=output_length,\n",
        "                                 batch_size = batch_size)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(bayesian_lstm.parameters(), lr=learning_rate)\n",
        "                               #  weight_decay=weight_decay)\n",
        "\n",
        "    bayesian_lstm.train()\n",
        "\n",
        "    print('fsfs')\n",
        "    for e in range(1, n_epochs+1):\n",
        "        for b in range(0, len(X_train), batch_size):\n",
        "            features = X_train[b:b+batch_size]\n",
        "            target = y_train[b:b+batch_size]\n",
        "\n",
        "            X_batch = torch.tensor(features,dtype=torch.float32)\n",
        "            y_batch = torch.tensor(target,dtype=torch.float32)\n",
        "\n",
        "            output = bayesian_lstm(X_batch)\n",
        "            loss = criterion(output.view(-1), y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if e % 2 == 0:\n",
        "          print('epoch', e, 'loss: ', loss.item())\n",
        "\n",
        "\n",
        "    offset = sequence_length\n",
        "\n",
        "    training_predictions = bayesian_lstm.predict(X_train)\n",
        "    #training_df = inverse_transform(training_predictions)\n",
        "    training_df = target_scaler.inverse_transform(training_predictions.reshape(-1,1))\n",
        "    training_truth_df = resample_df.iloc[offset:n_train + offset:1]\n",
        "\n",
        "    testing_predictions = bayesian_lstm.predict(X_test)\n",
        "    #testing_df = inverse_transform(testing_predictions)\n",
        "    testing_df = target_scaler.inverse_transform(testing_predictions.reshape(-1,1))\n",
        "    testing_truth_df = resample_df.iloc[n_train + offset::1]\n",
        "\n",
        "    y = np.array(testing_df)\n",
        "    yp = np.array(testing_truth_df)\n",
        "    ytr = np.array(training_df)\n",
        "    ytrp = np.array(training_truth_df)\n",
        "    err = mean_squared_error(y,yp)\n",
        "    print('mse err: ', err)\n",
        "    #err_train = mean_squared_error(ytr, ytrp)\n",
        "\n",
        "    return ytr,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YWlSH6A5tokX"
      },
      "outputs": [],
      "source": [
        "def sliding_windows(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O5LSilJitokZ"
      },
      "outputs": [],
      "source": [
        "class simLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(simLSTM, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        c_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        # Propagate input through LSTM\n",
        "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
        "\n",
        "        h_out = h_out.view(-1, self.hidden_size)\n",
        "\n",
        "        out = self.fc(h_out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4aGzm0Uttokb"
      },
      "outputs": [],
      "source": [
        "def singleLSTMmodel(DATASET, learning_rate=0.01, num_epochs=1000):\n",
        "\n",
        "  training_set = pd.read_csv(DATASET)\n",
        "\n",
        "  training_set = training_set.iloc[:, 0].values\n",
        "  training_set = np.expand_dims(training_set, 1)\n",
        "  sc = MinMaxScaler()\n",
        "  training_data = sc.fit_transform(training_set)\n",
        "\n",
        "  seq_length = 6\n",
        "  x, y = sliding_windows(training_data, seq_length)\n",
        "\n",
        "  train_size = int(len(y) * 0.7)\n",
        "  test_size = len(y) - train_size\n",
        "\n",
        "  dataX = Variable(torch.Tensor(np.array(x)))\n",
        "  dataY = Variable(torch.Tensor(np.array(y)))\n",
        "\n",
        "  trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
        "  trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
        "\n",
        "  testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
        "  testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
        "\n",
        "  input_size = 1\n",
        "  hidden_size = 64\n",
        "  num_layers = 1\n",
        "\n",
        "  num_classes = 1\n",
        "\n",
        "  lstm = simLSTM(num_classes, input_size, hidden_size, num_layers)\n",
        "\n",
        "  criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "  optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "  #optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      outputs = lstm(trainX)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # obtain the loss function\n",
        "      loss = criterion(outputs, trainY)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      if epoch % 25 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  lstm.eval()\n",
        "  train_predict = lstm(dataX)\n",
        "\n",
        "  data_predict = train_predict.data.numpy()\n",
        "  dataY_plot = dataY.data.numpy()\n",
        "\n",
        "  data_predict = sc.inverse_transform(data_predict)\n",
        "  dataY_plot = sc.inverse_transform(dataY_plot)\n",
        "\n",
        "  #plt.axvline(x=train_size, c='r', linestyle='--')\n",
        "\n",
        "  #plt.plot(dataY_plot)\n",
        "  #plt.plot(data_predict)\n",
        "  #plt.suptitle('Time-Series Prediction')\n",
        "  #plt.show()\n",
        "\n",
        "\n",
        "  print('mse : ' , mean_squared_error(lstm(testX).data.numpy(), testY.data.numpy()))\n",
        "\n",
        "  tf_trainX = sc.inverse_transform(lstm(trainX).data.numpy())\n",
        "  tf_testX = sc.inverse_transform(lstm(testX).data.numpy())\n",
        "\n",
        "  return tf_trainX, tf_testX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "blUHLVnxtokb"
      },
      "outputs": [],
      "source": [
        "class simGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(simGRU, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.lstm = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True, dropout = 0.4)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        c_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        # Propagate input through LSTM\n",
        "        ula, h_out = self.lstm(x, h_0)\n",
        "\n",
        "        h_out = h_out.view(-1, self.hidden_size)\n",
        "\n",
        "        out = self.fc(h_out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1mInF-Actokc"
      },
      "outputs": [],
      "source": [
        "def singleGRUmodel(DATASET, learning_rate=0.01, num_epochs=1000):\n",
        "\n",
        "  training_set = pd.read_csv(DATASET)\n",
        "\n",
        "  training_set = training_set.iloc[:, 0].values\n",
        "  training_set = np.expand_dims(training_set, 1)\n",
        "  sc = MinMaxScaler()\n",
        "  training_data = sc.fit_transform(training_set)\n",
        "\n",
        "  seq_length = 6\n",
        "  x, y = sliding_windows(training_data, seq_length)\n",
        "\n",
        "  train_size = int(len(y) * 0.7)\n",
        "  test_size = len(y) - train_size\n",
        "\n",
        "  dataX = Variable(torch.Tensor(np.array(x)))\n",
        "  dataY = Variable(torch.Tensor(np.array(y)))\n",
        "\n",
        "  trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
        "  trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
        "\n",
        "  testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
        "  testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
        "\n",
        "  input_size = 1\n",
        "  hidden_size = 128\n",
        "  num_layers = 1\n",
        "\n",
        "  num_classes = 1\n",
        "\n",
        "  lstm = simGRU(num_classes, input_size, hidden_size, num_layers)\n",
        "\n",
        "  criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "  optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "  #optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "      outputs = lstm(trainX)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # obtain the loss function\n",
        "      loss = criterion(outputs, trainY)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      if epoch % 100 == 0:\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "  lstm.eval()\n",
        "  train_predict = lstm(dataX)\n",
        "\n",
        "  data_predict = train_predict.data.numpy()\n",
        "  dataY_plot = dataY.data.numpy()\n",
        "\n",
        "  data_predict = sc.inverse_transform(data_predict)\n",
        "  dataY_plot = sc.inverse_transform(dataY_plot)\n",
        "\n",
        "  plt.axvline(x=train_size, c='r', linestyle='--')\n",
        "\n",
        "  plt.plot(dataY_plot)\n",
        "  plt.plot(data_predict)\n",
        "  plt.suptitle('Time-Series Prediction')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  print('mse : ' , mean_squared_error(testY.data.numpy(), lstm(testX).data.numpy()))\n",
        "  print('mae : ' , mean_absolute_error(testY.data.numpy(), lstm(testX).data.numpy()))\n",
        "  print('r2 : ' , r2_score(testY.data.numpy(), lstm(testX).data.numpy()))\n",
        "\n",
        "  tf_trainX = sc.inverse_transform(lstm(trainX).data.numpy())\n",
        "  tf_testX = sc.inverse_transform(lstm(testX).data.numpy())\n",
        "\n",
        "  return tf_trainX, tf_testX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BW6RBaXf4rZY"
      },
      "outputs": [],
      "source": [
        "def LSTMmodel2(filename):\n",
        "\n",
        "    resample_df = pd.read_csv(filename)\n",
        "    #resample_df = resample_df['tcm1yd']\n",
        "    #resample_df = energy_df['Appliances']\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(resample_df))\n",
        "    n_test = len(resample_df) - n_train\n",
        "\n",
        "\n",
        "    feature_array = resample_df.values\n",
        "\n",
        "    if str(type(feature_array[0]))!=\"<class 'numpy.ndarray'>\":\n",
        "        feature_array = np.expand_dims(feature_array, axis = -1)\n",
        "\n",
        "\t  # Fit Scaler only on Training features\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    feature_scaler.fit(feature_array[:n_train])\n",
        "    # Fit Scaler only on Training target values\n",
        "    target_scaler = MinMaxScaler()\n",
        "    target_scaler.fit(feature_array[:n_train, -1].reshape(-1, 1))\n",
        "\n",
        "    # Transfom on both Training and Test data\n",
        "    scaled_array = pd.DataFrame(feature_scaler.transform(feature_array))\n",
        "\n",
        "    sequence_length = 10\n",
        "    X, y = create_sliding_window(scaled_array, sequence_length)\n",
        "\n",
        "    X_train = X[:n_train]\n",
        "    y_train = y[:n_train]\n",
        "    X_test = X[n_train:]\n",
        "    y_test = y[n_train:]\n",
        "\n",
        "\n",
        "    #el mejor hasta ahora 256-32 (x2) dropout=0, lr=0.001, 1000 eps (0.039)\n",
        "\n",
        "    n_features = scaled_array.shape[-1]\n",
        "    sequence_length = 10\n",
        "    output_length = 1\n",
        "\n",
        "    batch_size = 128\n",
        "    n_epochs = 700\n",
        "    learning_rate = 0.01\n",
        "    #weight_decay = learning_rate/n_epochs\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    bayesian_lstm = BayesianDoubleLSTM(n_features=n_features,\n",
        "                                 output_length=output_length,\n",
        "                                 batch_size = batch_size)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(bayesian_lstm.parameters(), lr=learning_rate)\n",
        "                               #  weight_decay=weight_decay)\n",
        "\n",
        "    bayesian_lstm.train()\n",
        "\n",
        "    for e in range(1, n_epochs+1):\n",
        "        for b in range(0, len(X_train), batch_size):\n",
        "            features = X_train[b:b+batch_size]\n",
        "            target = y_train[b:b+batch_size]\n",
        "\n",
        "            X_batch = torch.tensor(features,dtype=torch.float32)\n",
        "            y_batch = torch.tensor(target,dtype=torch.float32)\n",
        "\n",
        "            output = bayesian_lstm(X_batch)\n",
        "            loss = criterion(output.view(-1), y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if e % 50 == 0:\n",
        "          print('epoch', e, 'loss: ', loss.item())\n",
        "\n",
        "\n",
        "    offset = sequence_length\n",
        "\n",
        "    training_predictions = bayesian_lstm.predict(X_train)\n",
        "    #training_df = inverse_transform(training_predictions)\n",
        "    training_df = target_scaler.inverse_transform(training_predictions.reshape(-1,1))\n",
        "    training_truth_df = resample_df.iloc[offset:n_train + offset:1]\n",
        "\n",
        "    testing_predictions = bayesian_lstm.predict(X_test)\n",
        "    #testing_df = inverse_transform(testing_predictions)\n",
        "    testing_df = target_scaler.inverse_transform(testing_predictions.reshape(-1,1))\n",
        "    testing_truth_df = resample_df.iloc[n_train + offset::1]\n",
        "\n",
        "    y = np.array(testing_df)\n",
        "    yp = np.array(testing_truth_df)\n",
        "    ytr = np.array(training_df)\n",
        "    ytrp = np.array(training_truth_df)\n",
        "    #err = mean_squared_error(y,yp)\n",
        "    #err_train = mean_squared_error(ytr, ytrp)\n",
        "\n",
        "    return ytr,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q0xRaWkh1m9w"
      },
      "outputs": [],
      "source": [
        "def RFmodel(filename):\n",
        "\t#filename = 'dataset_2.csv'\n",
        "    datos = read_csv(filename)\n",
        "    datos = datos[datos.columns[0]]\n",
        "    #datos=archivo[\"tcm1yd\"]\n",
        "    datos = [float(numeric_string) for numeric_string in datos]\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(datos))\n",
        "    n_test = len(datos) - n_train\n",
        "\n",
        "    data = series_to_supervised(datos, n_in=6)\n",
        "    trainX, trainY = data[:n_train, :-1], data[:n_train, -1]\n",
        "    testX, testY = data[n_train:, :-1], data[n_train:, -1]\n",
        "    model = RandomForestRegressor(n_estimators=1000)\n",
        "    model.fit(trainX, trainY)\n",
        "\n",
        "    yhat = model.predict(testX)\n",
        "    yhattrain = model.predict(trainX)\n",
        "    return yhattrain, yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pBahzWJL1qnA"
      },
      "outputs": [],
      "source": [
        "def GBmodel (filename):\n",
        "    #filename = 'dataset_2.csv'\n",
        "    datos = read_csv(filename)\n",
        "    datos = datos[datos.columns[0]]\n",
        "    #datos=archivo[\"Appliances\"]\n",
        "    datos = [float(numeric_string) for numeric_string in datos]\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(datos))\n",
        "    n_test = len(datos) - n_train\n",
        "\t # transform a time series dataset into a supervised learning dataset\n",
        "    n_in = 6\n",
        "    data = series_to_supervised(datos, n_in=n_in)\n",
        "    trainX_GB, trainY_GB = data[:n_train, :-1], data[:n_train, -1]\n",
        "    testX_GB, testY_GB = data[n_train:, :-1], data[n_train:, -1]\n",
        "    model = GradientBoostingRegressor(n_estimators=1000)\n",
        "    model.fit(trainX_GB, trainY_GB)\n",
        "\n",
        "    yhat_GB = model.predict(testX_GB)\n",
        "    yhattrain_GB = model.predict(trainX_GB)\n",
        "    return yhattrain_GB, yhat_GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "msVQxhSy2n4r"
      },
      "outputs": [],
      "source": [
        "def BRmodel (filename):\n",
        "    #filename = 'dataset_2.csv'\n",
        "    datos = read_csv(filename)\n",
        "    datos = datos[datos.columns[0]]\n",
        "    #datos=archivo[\"Appliances\"]\n",
        "    datos = [float(numeric_string) for numeric_string in datos]\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(datos))\n",
        "    n_test = len(datos) - n_train\n",
        "\t # transform a time series dataset into a supervised learning dataset\n",
        "    n_in = 6\n",
        "    data = series_to_supervised(datos, n_in=n_in)\n",
        "    trainX_GB, trainY_GB = data[:n_train, :-1], data[:n_train, -1]\n",
        "    testX_GB, testY_GB = data[n_train:, :-1], data[n_train:, -1]\n",
        "    model = BaggingRegressor(n_estimators=1000)\n",
        "    model.fit(trainX_GB, trainY_GB)\n",
        "\n",
        "    yhat_BR = model.predict(testX_GB)\n",
        "    yhattrain_BR = model.predict(trainX_GB)\n",
        "    return yhattrain_BR, yhat_BR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mtckt8jp1jnZ"
      },
      "outputs": [],
      "source": [
        "def KNNmodel (filename):\n",
        "    #filename = 'dataset_2.csv'\n",
        "    datos = read_csv(filename)\n",
        "    datos = datos[datos.columns[0]]\n",
        "    #datos=archivo[\"Appliances\"]\n",
        "    datos = [float(numeric_string) for numeric_string in datos]\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(datos))\n",
        "    n_test = len(datos) - n_train\n",
        "\t # transform a time series dataset into a supervised learning dataset\n",
        "    n_in = 6\n",
        "    data = series_to_supervised(datos, n_in=n_in)\n",
        "    trainX_GB, trainY_GB = data[:n_train, :-1], data[:n_train, -1]\n",
        "    testX_GB, testY_GB = data[n_train:, :-1], data[n_train:, -1]\n",
        "    model = KNeighborsRegressor(n_neighbors=3)\n",
        "    model.fit(trainX_GB, trainY_GB)\n",
        "\n",
        "    yhat_KNN = model.predict(testX_GB)\n",
        "    yhattrain_KNN = model.predict(trainX_GB)\n",
        "    return yhattrain_KNN, yhat_KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6XXmQxGv1rLn"
      },
      "outputs": [],
      "source": [
        "def ARIMAmodel(filename):\n",
        "    datos = pd.read_csv(filename)\n",
        "    #datos=archivo[\"Appliances\"]\n",
        "    datos = datos[datos.columns[0]]\n",
        "    data = [float(numeric_string) for numeric_string in datos]\n",
        "\n",
        "    train_split = 0.7\n",
        "    n_train = int(train_split * len(datos))\n",
        "    n_test = len(datos) - n_train\n",
        "\n",
        "    train, test =data[:n_train], data[n_train:]\n",
        "    test = test[:n_test]\n",
        "    # Fit a simple auto_arima model\n",
        "    arima = pm.auto_arima(train, error_action='ignore', trace=True,\n",
        "                          suppress_warnings=True, maxiter=5,\n",
        "                          seasonal=False, m=12)\n",
        "\n",
        "    pred_arima_train=arima.predict_in_sample()\n",
        "    new_model = deepcopy(arima)\n",
        "    new_model.method = \"nm\"\n",
        "    new_model.fit(test, maxiter=0, start_params=new_model.params())\n",
        "    pred_arima_test=new_model.predict_in_sample()\n",
        "    pred_arima_test = pred_arima_test[6:]\n",
        "    return pred_arima_train, pred_arima_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ordering & Weight calculation"
      ],
      "metadata": {
        "id": "7EvPl5deuptt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dE3AQzvZ1xSp"
      },
      "outputs": [],
      "source": [
        "def ordenar(X,y): #X es el array con todas las predicciones, modelos en filas, >\n",
        "    n = len(X[0])\n",
        "    Z = X.copy()\n",
        "    for i in range(1,n):\n",
        "        O = np.maximum(1-np.abs(X[:,i-1]-y[i-1])/np.abs(y[i-1]),0)\n",
        "        SO = np.argsort(O)\n",
        "        for k in SO:\n",
        "            Z[k,i] = np.mean(X[SO[k],i])#Esto es porque se coge la media si hay>\n",
        "    return(Z[:,1:]) #el primer tiempo me lo cargo (no lo puedo ordenar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cy3hY6kitokh"
      },
      "outputs": [],
      "source": [
        "def ordenarPorValor(X): #X es el array con todas las predicciones, modelos en filas, >\n",
        "    n = len(X[0])\n",
        "    Z = X.copy()\n",
        "    for i in range(n):\n",
        "        O = np.maximum(X[:,i-1],0)\n",
        "        SO = np.argsort(O)\n",
        "        for k in SO:\n",
        "            Z[k,i] = np.mean(X[SO[k],i])#Esto es porque se coge la media si hay>\n",
        "    return(Z) #el primer tiempo me lo cargo (no lo puedo ordenar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0PGWWl5k10oo"
      },
      "outputs": [],
      "source": [
        "def ECM(w,Z,y):\n",
        "    ECM = 0\n",
        "    y = np.transpose(y)\n",
        "    for i in range(len(y)):\n",
        "        ECM += (np.dot(w,Z[:,i])-y[i])**2\n",
        "    return(ECM/len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CUql-e0q12jw"
      },
      "outputs": [],
      "source": [
        "def pesosIOWA(X,y):\n",
        "    Z = ordenar(X,y)\n",
        "    m = len(Z)\n",
        "    bounds = Bounds([0]*m,[1]*m)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(Z,np.transpose(y[1:])), options={'maxiter': 1000}, bounds=bounds, constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-SHp7flItoki"
      },
      "outputs": [],
      "source": [
        "def pesosOWA(X,y):\n",
        "    Z = ordenarPorValor(X)\n",
        "    m = len(Z)\n",
        "    bounds = Bounds([0]*m,[1]*m)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(Z,np.transpose(y)), options={'maxiter': 1000}, bounds=bounds, constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pesosOWA1(X,y):\n",
        "    Z = ordenarPorValor(X)\n",
        "    m = len(Z)\n",
        "    bounds = Bounds([0]*m,[1]*m)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(Z,np.transpose(y[1:])), options={'maxiter': 1000}, bounds=bounds, constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ],
      "metadata": {
        "id": "UbzKl6oevpCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sPj6ibustokj"
      },
      "outputs": [],
      "source": [
        "def pesosNOWA(X,y):\n",
        "    Z = ordenarPorValor(X)\n",
        "    m = len(Z)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(Z,np.transpose(y)), options={'maxiter': 1000},constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qB-3F8_Etokj"
      },
      "outputs": [],
      "source": [
        "def pesosNWAM(X,y):\n",
        "    #Z = ordenar(X,y)\n",
        "    m = len(X)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(X,np.transpose(y)), options={'maxiter': 1000}, constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AKXFS1FYtokk"
      },
      "outputs": [],
      "source": [
        "def pesosWAM(X,y):\n",
        "    #Z = ordenar(X,y)\n",
        "    m = len(X)\n",
        "    bounds = Bounds([0]*m,[1]*m)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOWA = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(X,np.transpose(y)), options={'maxiter': 1000}, bounds=bounds, constraints=linear_constraint)\n",
        "    return(res_IOWA.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bA6za1tp15jA"
      },
      "outputs": [],
      "source": [
        "def pesosIOLF(X,y):\n",
        "    Z = ordenar(X,y)\n",
        "    m = len(Z)\n",
        "    linear_constraint = LinearConstraint([1]*m, [1], [1])\n",
        "    x0 = np.array([1./m]*m)\n",
        "    res_IOLF = minimize(ECM, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(), args=(Z,np.transpose(y[1:])), options={'verbose': 1}, constraints=linear_constraint)\n",
        "    return(res_IOLF.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliar functions"
      ],
      "metadata": {
        "id": "uyqlvFUuv0f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VhzTGIfXtokl"
      },
      "outputs": [],
      "source": [
        "def read_data(Folder,ith):\n",
        "    dataset_name = str(Folder) + \"/\" + str(ith) + \".csv\"\n",
        "    data = read_csv(dataset_name)\n",
        "    data = data[data.columns[0]]\n",
        "    data = [float(numeric_string) for numeric_string in data]\n",
        "    return data, dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1TnraRdHtokl"
      },
      "outputs": [],
      "source": [
        "def split_data(data, train_split=0.7):\n",
        "    n_train = int(train_split * len(data))\n",
        "    n_test = len(data) - n_train\n",
        "\n",
        "    data = series_to_supervised(data, n_in=6)\n",
        "    trainX, ytrain = data[:n_train, :-1], data[:n_train, -1]\n",
        "    testX, ytest = data[n_train:, :-1], data[n_train:, -1]\n",
        "\n",
        "    return ytrain, ytest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iXK-jugytokl"
      },
      "outputs": [],
      "source": [
        "def predictions(dataset_name):\n",
        "    preds = [RFmodel(dataset_name), GBmodel(dataset_name), ARIMAmodel(dataset_name), KNNmodel(dataset_name),\n",
        "            BRmodel(dataset_name), singleLSTMmodel(dataset_name, num_epochs=1200), singleGRUmodel(dataset_name, num_epochs=1200)]\n",
        "\n",
        "    aux1 = preds[5][0].squeeze(axis = 1)\n",
        "    aux2 = preds[5][1].squeeze(axis = 1)\n",
        "    aux3 = preds[6][0].squeeze(axis = 1)\n",
        "    aux4 = preds[6][1].squeeze(axis = 1)\n",
        "\n",
        "    preds[5] = [aux1, aux2]\n",
        "    preds[6] = [aux3, aux4]\n",
        "\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dMRopS8mtokm"
      },
      "outputs": [],
      "source": [
        "def stack_predictions(preds):\n",
        "    Xtrain = np.vstack((preds[0][0][:-5] , preds[1][0][:-5], preds[2][0][:-5], preds[3][0][:-5], preds[4][0][:-5], preds[5][0], preds[6][0]))\n",
        "    # Xtest = np.vstack((preds[0][1][4:], preds[1][1][4:], preds[2][1][4:], preds[3][1][4:], preds[4][1][4:], preds[5][1], preds[6][1]))\n",
        "    Xtest = np.vstack((preds[0][1], preds[1][1], preds[2][1], preds[3][1], preds[4][1], preds[5][1][4:], preds[6][1][4:]))\n",
        "    # Xtest = np.vstack((preds[0][1][4:], preds[1][1], preds[2][1][4:], preds[3][1][4:], preds[4][1][4:], preds[5][1], preds[6][1]))\n",
        "    return Xtrain, Xtest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XPgIWr5ptok9"
      },
      "outputs": [],
      "source": [
        "def Xy_transform(Xtrain, ytrain, Xtest, ytest):\n",
        "    Xtrain_transformed = []\n",
        "    ytrain_transformed = ytrain[:-12]\n",
        "    for i in range(0,7):\n",
        "        if i == 2:\n",
        "            Xtrain_transformed.append(Xtrain[i][12-5:])\n",
        "        else:\n",
        "            Xtrain_transformed.append(Xtrain[i][1:-6])\n",
        "    Xtest_transformed = []\n",
        "    ytest_transformed = ytest[:-2]\n",
        "    for i in range(0,7):\n",
        "        if i == 5 or i==6:\n",
        "            Xtest_transformed.append(Xtest[i][2:])\n",
        "        else:\n",
        "            Xtest_transformed.append(Xtest[i][1:-1])\n",
        "    return np.array(Xtrain_transformed), ytrain_transformed, np.array(Xtest_transformed), ytest_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Errors\n"
      ],
      "metadata": {
        "id": "3uwP5FchwG2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1l5Ok0jdtokm"
      },
      "outputs": [],
      "source": [
        "def individual_errors(preds, ytest):\n",
        "    MSE = []\n",
        "    MAE = []\n",
        "    R2 = []\n",
        "    #a = preds[1][1].squeeze(axis = 1)\n",
        "    #preds[1] = [preds[1][0], a]\n",
        "    for i in range(5):\n",
        "        if i == 9:\n",
        "            MSE.append(mean_squared_error(ytest[5:], preds[i][1][1:]))\n",
        "            MAE.append(mean_absolute_error(ytest[5:], preds[i][1][1:]))\n",
        "            R2.append(r2_score(ytest[5:], preds[i][1][1:]))\n",
        "        else:\n",
        "            MSE.append(mean_squared_error(ytest, preds[i][1]))\n",
        "            MAE.append(mean_absolute_error(ytest, preds[i][1]))\n",
        "            R2.append(r2_score(ytest, preds[i][1]))\n",
        "    for j in range(5,7):\n",
        "        MSE.append(mean_squared_error(ytest, preds[j][1][4:]))\n",
        "        MAE.append(mean_absolute_error(ytest, preds[j][1][4:]))\n",
        "        R2.append(r2_score(ytest, preds[j][1][4:]))\n",
        "\n",
        "    errors = {'MSE': MSE, 'MAE': MAE, 'R2': R2}\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "McewOrw_tokm"
      },
      "outputs": [],
      "source": [
        "def individual_errors_byX(Xtest_transformed, ytest_transformed):\n",
        "    MSE = []\n",
        "    MAE = []\n",
        "    R2 = []\n",
        "    for i in range(0,7):\n",
        "        MSE.append(mean_squared_error(ytest_transformed, Xtest_transformed[i]))\n",
        "        MAE.append(mean_absolute_error(ytest_transformed, Xtest_transformed[i]))\n",
        "        R2.append(r2_score(ytest_transformed, Xtest_transformed[i] ))\n",
        "    errors = {'MSE': MSE, 'MAE': MAE, 'R2': R2}\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WZP3bttqtok2"
      },
      "outputs": [],
      "source": [
        "def time_ensemble_error(weights, Ztest, ytest, operator, err_type='MSE'):\n",
        "\n",
        "    if err_type == 'MSE':\n",
        "        err = mean_squared_error(np.dot(weights[operator], Ztest), ytest  )\n",
        "\n",
        "    if err_type == 'MAE':\n",
        "        err = mean_absolute_error(np.dot(weights[operator], Ztest), ytest)\n",
        "\n",
        "    if err_type == 'R2':\n",
        "        err = r2_score(np.dot(weights[operator], Ztest), ytest)\n",
        "\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QP_asF_rtok3"
      },
      "outputs": [],
      "source": [
        "def ensemble_errors(weights, Ztest, Xtest, ytest):\n",
        "    Zo = Ztest\n",
        "    yo = ytest[1:]\n",
        "    Z = Xtest\n",
        "    y = ytest\n",
        "    C = ordenarPorValor(Xtest)\n",
        "    ensemble_errors = {\n",
        "        'MSE': {\n",
        "            'IOLF': time_ensemble_error(weights, Zo, yo, 'IOLF'),\n",
        "            'IOWA': time_ensemble_error(weights, Zo, yo, 'IOWA'),\n",
        "            'WAM':  time_ensemble_error(weights, Z, y, 'WAM'),\n",
        "            'OWA': time_ensemble_error(weights, C, y, 'OWA'),\n",
        "            'NWAM':  time_ensemble_error(weights, Z, y, 'NWAM'),\n",
        "            'NOWA': time_ensemble_error(weights, C, y, 'NOWA')\n",
        "        },\n",
        "        'MAE': {\n",
        "            'IOLF': time_ensemble_error(weights, Zo, yo, 'IOLF', 'MAE'),\n",
        "            'IOWA': time_ensemble_error(weights, Zo, yo, 'IOWA', 'MAE'),\n",
        "            'WAM':  time_ensemble_error(weights, Z, y, 'WAM', 'MAE'),\n",
        "            'OWA': time_ensemble_error(weights, C, y, 'OWA', 'MAE'),\n",
        "            'NWAM':  time_ensemble_error(weights, Z, y, 'NWAM', 'MAE'),\n",
        "            'NOWA': time_ensemble_error(weights, C, y, 'NOWA', 'MAE')\n",
        "        },\n",
        "        'R2': {\n",
        "            'IOLF': time_ensemble_error(weights, Zo, yo, 'IOLF', 'R2'),\n",
        "            'IOWA': time_ensemble_error(weights, Zo, yo, 'IOWA', 'R2'),\n",
        "            'WAM':  time_ensemble_error(weights, Z, y, 'WAM', 'R2'),\n",
        "            'OWA': time_ensemble_error(weights, C, y, 'OWA', 'R2'),\n",
        "            'NWAM':  time_ensemble_error(weights, Z, y, 'NWAM', 'R2'),\n",
        "            'NOWA': time_ensemble_error(weights, C, y, 'NOWA', 'R2')\n",
        "        }\n",
        "    }\n",
        "    return ensemble_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DIn8j_lXtok5"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "def err_val (errors):\n",
        "    vals = list(errors.values())\n",
        "    vals[0][7] = list(vals[0][7].values())\n",
        "    vals[1][7] = list(vals[1][7].values())\n",
        "    vals[2][7] = list(vals[2][7].values())\n",
        "    return list(itertools.chain(vals[0][:7],vals[0][7], vals[1][:7],vals[1][7], vals[2][:7],vals[2][7] ))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "31woEAeMwRH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmgGNPWUtolH"
      },
      "outputs": [],
      "source": [
        "weights_IOLF = []\n",
        "weights_IOWA = []\n",
        "operators =['IOLF', 'IOWA', 'WAM', 'OWA', 'NWAM', 'NOWA']\n",
        "MSE_IOLF = []\n",
        "MSE_IOWA = []\n",
        "MAE_IOLF = []\n",
        "MAE_IOWA = []\n",
        "errors = []\n",
        "ds_pos = []\n",
        "\n",
        "ds_ini = 33\n",
        "ds_fin = 50\n",
        "\n",
        "excel = xls.Workbook('Aligned-results-' + str(ds_ini) + '-' + str(ds_fin) + '.xlsx')\n",
        "sheet_test = excel.add_worksheet()\n",
        "sheet_test.set_column('A:A', 20)\n",
        "sheet_train = excel.add_worksheet()\n",
        "sheet_train.set_column('A:A', 20)\n",
        "\n",
        "errors_matrix = []\n",
        "train_errors_matrix = []\n",
        "err_types = ['MSE', 'MAE', 'R2']\n",
        "header = []\n",
        "\n",
        "for j in range(len(err_types)):\n",
        "    for i in range(7+len(operators)):\n",
        "        if i<7:\n",
        "            s = err_types[j] + ' ' + str(i)\n",
        "            header.append(s)\n",
        "        else:\n",
        "            s = err_types[j] + ' ' + operators[i - 7]\n",
        "            header.append(s)\n",
        "header_vec = [{'header': header[i]} for i in range(len(header))]\n",
        "\n",
        "# range_datasets = [i for i in chain(range(1,5), range(25,29), range(33,50))]\n",
        "\n",
        "for dataset in range(ds_ini, ds_fin +1):\n",
        "    for reps in range(5):\n",
        "\n",
        "        data, dataset_name = read_data('Datasets', dataset)\n",
        "\n",
        "        ytrain, ytest = split_data(data)\n",
        "\n",
        "        preds = predictions(dataset_name)\n",
        "\n",
        "        Xtrain, Xtest = stack_predictions(preds)\n",
        "\n",
        "        Xtrain_transformed, ytrain_transformed, Xtest_transformed, ytest_transformed = Xy_transform(Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "        weights = {\n",
        "                    'IOLF': pesosIOLF(Xtrain_transformed, ytrain_transformed),\n",
        "                    'IOWA': pesosIOWA(Xtrain_transformed, ytrain_transformed),\n",
        "                    'WAM':  pesosWAM(Xtrain_transformed, ytrain_transformed),\n",
        "                    'OWA':  pesosOWA(Xtrain_transformed, ytrain_transformed),\n",
        "                    'NWAM': pesosNWAM(Xtrain_transformed, ytrain_transformed),\n",
        "                    'NOWA': pesosNOWA(Xtrain_transformed, ytrain_transformed)\n",
        "                    }\n",
        "\n",
        "\n",
        "        Ztest, Ztrain = ordenar(Xtest_transformed, ytest_transformed), ordenar(Xtrain_transformed, ytrain_transformed)\n",
        "        # TEST ERRORS\n",
        "        errors = individual_errors_byX(Xtest_transformed, ytest_transformed)\n",
        "        ens_errors = ensemble_errors(weights, Ztest, Xtest_transformed, ytest_transformed)\n",
        "        errors['MSE'].append(ens_errors['MSE'])\n",
        "        errors['MAE'].append(ens_errors['MAE'])\n",
        "        errors['R2'].append(ens_errors['R2'])\n",
        "\n",
        "        errors_matrix.append(err_val(errors))\n",
        "\n",
        "        # TRAIN ERRORS\n",
        "        train_errors = individual_errors_byX(Xtrain_transformed, ytrain_transformed)\n",
        "        train_ens_errors = ensemble_errors(weights, Ztrain, Xtrain_transformed, ytrain_transformed)\n",
        "        train_errors['MSE'].append(train_ens_errors['MSE'])\n",
        "        train_errors['MAE'].append(train_ens_errors['MAE'])\n",
        "        train_errors['R2'].append(train_ens_errors['R2'])\n",
        "\n",
        "        train_errors_matrix.append(err_val(train_errors))\n",
        "\n",
        "\n",
        "        weights_IOLF.append(weights['IOLF'])\n",
        "        weights_IOWA.append(weights['IOWA'])\n",
        "\n",
        "\n",
        "        ds_pos.append(dataset_name)\n",
        "\n",
        "\n",
        "        #sheet.write(1, 0, err_MSE_IOLF)\n",
        "        #sheet.write(1, 1, err_MSE_IOWA)\n",
        "        #sheet.write(1, 2, err_MAE_IOLF)\n",
        "        #sheet.write(1, 3, err_MAE_IOWA)\n",
        "        print('Dataset ' + str(dataset))\n",
        "        print('Test: ')\n",
        "        print(errors_matrix)\n",
        "        print('Train: ')\n",
        "        print(train_errors_matrix)\n",
        "        print(str(weights['IOLF']), str(weights['IOWA']))\n",
        "\n",
        "\n",
        "sheet_test.add_table('A1:AM'+str(len(errors_matrix)+1),\n",
        "                {'data': errors_matrix,\n",
        "                 'columns': header_vec} )\n",
        "\n",
        "sheet_train.add_table('A1:AM'+str(len(train_errors_matrix)+1),\n",
        "                {'data': train_errors_matrix,\n",
        "                 'columns': header_vec} )\n",
        "excel.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNcY5uDo16DQ"
      },
      "outputs": [],
      "source": [
        "'''weights_IOLF = []\n",
        "weights_IOWA = []\n",
        "operators =['IOLF', 'IOWA', 'WAM', 'OWA']\n",
        "MSE_IOLF = []\n",
        "MSE_IOWA = []\n",
        "MAE_IOLF = []\n",
        "MAE_IOWA = []\n",
        "errors = []\n",
        "ds_pos = []\n",
        "\n",
        "excel = xls.Workbook('results.xlsx')\n",
        "sheet = excel.add_worksheet()\n",
        "sheet.set_column('A:A', 20)\n",
        "\n",
        "errors_matrix = []\n",
        "err_types = ['MSE', 'MAE', 'R2']\n",
        "header = []\n",
        "for j in range(len(err_types)):\n",
        "    for i in range(7+len(operators)):\n",
        "        if i<7:\n",
        "            s = err_types[j] + ' ' + str(i)\n",
        "            header.append(s)\n",
        "        else:\n",
        "            s = err_types[j] + ' ' + operators[i - 7]\n",
        "            header.append(s)\n",
        "header_vec = [{'header': header[i]} for i in range(len(header))]\n",
        "\n",
        "for dataset in range(33,38):\n",
        "    for reps in range(1):\n",
        "\n",
        "        data, dataset_name = read_data('Datasets', dataset)\n",
        "\n",
        "        ytrain, ytest = split_data(data)\n",
        "\n",
        "        preds = predictions(dataset_name)\n",
        "\n",
        "        Xtrain, Xtest = stack_predictions(preds)\n",
        "\n",
        "        weights = {\n",
        "                    'IOLF': pesosIOLF(Xtrain, ytrain[:-5]),\n",
        "                    'IOWA': pesosIOWA(Xtrain, ytrain[:-5]),\n",
        "                    'WAM':  pesosWAM(Xtrain, ytrain[:-5]),\n",
        "                    'OWA':  pesosOWA(Xtrain, ytrain[:-5])\n",
        "                    }\n",
        "\n",
        "\n",
        "        Ztest, Ztrain = ordenar(Xtest, ytest), ordenar(Xtrain, ytrain)\n",
        "\n",
        "        errors = individual_errors(preds, ytest)\n",
        "        ens_errors = ensemble_errors(weights, Ztest, ytest)\n",
        "        errors['MSE'].append(ens_errors['MSE'])\n",
        "        errors['MAE'].append(ens_errors['MAE'])\n",
        "        errors['R2'].append(ens_errors['R2'])\n",
        "\n",
        "        errors_matrix.append(err_val(errors))\n",
        "\n",
        "        weights_IOLF.append(weights['IOLF'])\n",
        "        weights_IOWA.append(weights['IOWA'])\n",
        "\n",
        "\n",
        "        ds_pos.append(dataset_name)\n",
        "\n",
        "\n",
        "        #sheet.write(1, 0, err_MSE_IOLF)\n",
        "        #sheet.write(1, 1, err_MSE_IOWA)\n",
        "        #sheet.write(1, 2, err_MAE_IOLF)\n",
        "        #sheet.write(1, 3, err_MAE_IOWA)\n",
        "        print(errors_matrix)\n",
        "        print(str(weights['IOLF']), str(weights['IOWA']))\n",
        "\n",
        "\n",
        "sheet.add_table('A1:AG'+str(len(errors_matrix)+1),\n",
        "                {'data': errors_matrix,\n",
        "                 'columns': header_vec} )\n",
        "excel.close()'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b00aa13431af96271f81de7e6c74dda21679ca55c25a672a9b8c84aa9b0641a0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
